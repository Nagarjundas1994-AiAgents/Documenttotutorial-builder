{
  "title": "Getting Started with LangGraph: Building Stateful Agents",
  "generated_at": "2025-07-03T15:41:30.957549",
  "source_query": "Create a tutorial from the documentation at https://langchain-ai.github.io/langgraph/",
  "outline": {
    "title": "Getting Started with LangGraph: Building Stateful Agents",
    "sections": [
      {
        "title": "Introduction to LangGraph",
        "brief_description": "Overview of LangGraph's purpose, core benefits, and ecosystem positioning"
      },
      {
        "title": "Installation and Setup",
        "brief_description": "How to install LangGraph and verify your installation"
      },
      {
        "title": "Creating Your First Agent",
        "brief_description": "Step-by-step guide to building a simple agent using prebuilt components"
      },
      {
        "title": "Understanding Graph Concepts",
        "brief_description": "Explanation of core concepts: State, Nodes, Edges, and Reducers"
      },
      {
        "title": "Building Custom Workflows",
        "brief_description": "How to create and compile custom agent workflows with state management"
      },
      {
        "title": "Advanced Features",
        "brief_description": "Exploring durable execution, human-in-the-loop, and memory capabilities"
      },
      {
        "title": "Deployment and Monitoring",
        "brief_description": "Guidance on deploying agents and using LangSmith for observability"
      }
    ]
  },
  "sections": [
    {
      "title": "Introduction to LangGraph",
      "brief_description": "Overview of LangGraph's purpose, core benefits, and ecosystem positioning",
      "content": "```markdown\n# Introduction to LangGraph\n\n## What is LangGraph?\n\nLangGraph is a powerful orchestration framework designed for building stateful, long-running agents and workflows. Trusted by companies like Klarna, Replit, and Elastic, it provides the foundation for creating sophisticated AI systems that can maintain context, handle complex tasks, and interact with users over extended periods.\n\n## Core Benefits\n\nLangGraph offers several key advantages for agent development:\n\n1. **Durable Execution**: Build agents that persist through failures and can run for extended periods, automatically resuming from where they left off.\n\n2. **Human-in-the-Loop**: Seamlessly incorporate human oversight by inspecting and modifying agent state at any point during execution.\n\n3. **Comprehensive Memory**: Create truly stateful agents with both short-term working memory and long-term persistent memory across sessions.\n\n4. **Production-Ready**: Deploy sophisticated agent systems with scalable infrastructure designed for stateful workflows.\n\n## Ecosystem Positioning\n\nWhile LangGraph can be used standalone, it integrates seamlessly with the broader LangChain ecosystem:\n\n- **LangChain**: Provides integrations and composable components for LLM application development\n- **LangSmith**: Offers debugging and observability tools for complex agent behavior\n- **LangGraph Platform**: A deployment platform for managing and scaling agents in production\n\nLangGraph is particularly well-suited for applications requiring:\n- Multi-step reasoning\n- Dynamic workflow adaptation\n- Persistent conversation history\n- Human approval workflows\n- Integration with external tools and APIs\n\n## Key Concepts\n\nAt its core, LangGraph models agent workflows as graphs composed of:\n- **State**: A shared data structure representing the current system snapshot\n- **Nodes**: Functions containing your agent's logic\n- **Edges**: Connections that determine the flow between nodes\n\nThis architecture enables you to build complex, looping workflows that evolve state over time while maintaining full control over your agent's behavior.\n```"
    },
    {
      "title": "Installation and Setup",
      "brief_description": "How to install LangGraph and verify your installation",
      "content": "```markdown\n# Installation and Setup\n\n## Prerequisites\n\nBefore installing LangGraph, ensure you have:\n- Python 3.11 or later\n- pip package manager\n- An API key for your preferred LLM provider (Anthropic, OpenAI, etc.)\n\n## Installing LangGraph\n\nInstall the core LangGraph package using pip:\n\n```bash\npip install -U langgraph\n```\n\nFor full functionality including LLM integrations, install with optional dependencies:\n\n```bash\npip install -U \"langgraph[anthropic]\"  # For Anthropic models\n# or\npip install -U \"langgraph[openai]\"     # For OpenAI models\n```\n\n## Verifying Your Installation\n\nAfter installation, verify everything works by creating a simple agent:\n\n```python\nfrom langgraph.prebuilt import create_react_agent\n\n# Simple tool example\ndef dummy_tool(query: str) -> str:\n    return f\"Processed: {query}\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[dummy_tool],\n    prompt=\"You are a helpful assistant\"\n)\n\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"test message\"}]}\n)\nprint(response)\n```\n\n## Configuration\n\nSet your API keys as environment variables:\n\n```bash\nexport ANTHROPIC_API_KEY='your-api-key'\n# or\nexport OPENAI_API_KEY='your-api-key'\n```\n\n## Troubleshooting\n\nIf you encounter issues:\n1. Check Python version (`python --version`)\n2. Verify pip is up to date (`pip install --upgrade pip`)\n3. Ensure your API keys are properly set\n4. Consult the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) for version-specific notes\n\nFor production environments, consider using a dependency manager like `uv` for faster, more reliable installations:\n\n```bash\nuv pip install --python=3.11 \"langgraph[anthropic]\"\n```\n\nNow you're ready to start building with LangGraph!\n```"
    },
    {
      "title": "Creating Your First Agent",
      "brief_description": "Step-by-step guide to building a simple agent using prebuilt components",
      "content": "# Creating Your First Agent\n\nIn this section, we'll walk through building your first agent using LangGraph's prebuilt components. This will give you a hands-on introduction to creating stateful agents without needing to build everything from scratch.\n\n## Prerequisites\n\nBefore we begin, ensure you have:\n1. Python 3.11+ installed\n2. LangGraph installed (`pip install -U langgraph`)\n3. An Anthropic API key (or other supported LLM provider)\n\n## Step 1: Import Required Components\n\n```python\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain.chat_models import init_chat_model\n```\n\n## Step 2: Define Your Tools\n\nTools are functions your agent can call to perform actions. Let's create a simple weather lookup tool:\n\n```python\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    if city.lower() == \"san francisco\":\n        return \"Sunny with a high of 65Â°F\"\n    return f\"Weather information unavailable for {city}\"\n```\n\n## Step 3: Initialize Your LLM\n\n```python\n# For Anthropic (requires langchain[anthropic])\nllm = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\", temperature=0)\n```\n\n## Step 4: Create the Agent\n\nCombine your LLM and tools into an agent:\n\n```python\nagent = create_react_agent(\n    model=llm,\n    tools=[get_weather],\n    prompt=\"You are a helpful weather assistant\"\n)\n```\n\n## Step 5: Run Your Agent\n\nInvoke the agent with a user message:\n\n```python\nresponse = agent.invoke({\n    \"messages\": [{\n        \"role\": \"user\", \n        \"content\": \"What's the weather in San Francisco?\"\n    }]\n})\n```\n\n## Understanding the Response\n\nThe agent will return a dictionary containing:\n- `messages`: The conversation history\n- Potentially a `structured_response` if configured\n- Any custom state fields you've defined\n\n## Adding Memory (Optional)\n\nTo enable multi-turn conversations, add a checkpointer:\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\nagent = create_react_agent(\n    model=llm,\n    tools=[get_weather],\n    checkpointer=checkpointer\n)\n\n# Use thread_id to maintain conversation state\nconfig = {\"configurable\": {\"thread_id\": \"123\"}}\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi!\"}]}, config)\n```\n\n## Next Steps\n\nYou've now created a basic agent! In the next sections, we'll explore:\n- Customizing the agent's behavior\n- Building more complex workflows\n- Adding advanced features like human review\n\nTry experimenting with different tools and prompts to see how they affect your agent's behavior."
    },
    {
      "title": "Understanding Graph Concepts",
      "brief_description": "Explanation of core concepts: State, Nodes, Edges, and Reducers",
      "content": "# Understanding Graph Concepts\n\nLangGraph models agent workflows as graphs composed of three fundamental components that work together to create stateful, controllable agents. Let's explore each concept in detail:\n\n## State: The Shared Memory\n\nThe `State` is a shared data structure representing your application's current snapshot:\n- Typically defined as a `TypedDict` or Pydantic `BaseModel`\n- Contains all variables your nodes need to access and modify\n- Persists throughout the execution of your graph\n\n```python\nfrom typing import Annotated, TypedDict\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.message import add_messages\n\nclass AgentState(TypedDict):\n    messages: Annotated[list, add_messages]  # Message history\n    user_preferences: dict  # User-specific settings\n    session_data: dict  # Temporary session information\n```\n\n## Nodes: The Workers\n\nNodes are Python functions that contain your agent's logic:\n- Receive the current `State` as input\n- Perform computations or side effects\n- Return updates to the `State`\n\n```python\ndef process_user_input(state: AgentState):\n    \"\"\"Node to handle user input\"\"\"\n    last_message = state[\"messages\"][-1]\n    processed = analyze_message(last_message)\n    return {\"messages\": [processed], \"session_data\": {\"last_processed\": now()}}\n```\n\n## Edges: The Decision Makers\n\nEdges determine the flow between nodes:\n- **Normal edges**: Fixed transitions from one node to another\n- **Conditional edges**: Dynamic routing based on state\n\n```python\nfrom langgraph.graph import StateGraph\n\nbuilder = StateGraph(AgentState)\n\n# Add nodes\nbuilder.add_node(\"process_input\", process_user_input)\nbuilder.add_node(\"generate_response\", generate_response)\n\n# Fixed edge\nbuilder.add_edge(\"process_input\", \"generate_response\")\n\n# Conditional edge\ndef should_escalate(state):\n    if state[\"session_data\"].get(\"needs_human\"):\n        return \"human_review\"\n    return \"generate_response\"\n\nbuilder.add_conditional_edges(\"process_input\", should_escalate)\n```\n\n## Reducers: State Update Handlers\n\nReducers control how state updates are applied:\n- Each state key can have its own reducer\n- Default is overwrite behavior\n- Custom reducers enable merging strategies\n\n```python\nfrom operator import add\n\nclass AgentState(TypedDict):\n    message_count: Annotated[int, add]  # Will sum values\n    message_history: Annotated[list, lambda x,y: x + y]  # Will concatenate\n    current_status: str  # Will overwrite\n```\n\n### Key Takeaways:\n1. **State** is your application's shared memory\n2. **Nodes** contain your business logic\n3. **Edges** control workflow routing\n4. **Reducers** manage state updates\n\nThese components work together to create flexible, stateful agent workflows that can handle complex, multi-step processes while maintaining full control over execution flow."
    },
    {
      "title": "Building Custom Workflows",
      "brief_description": "How to create and compile custom agent workflows with state management",
      "content": "# Building Custom Workflows\n\nLangGraph's true power emerges when you move beyond prebuilt agents and start creating custom workflows tailored to your specific needs. This section will guide you through building stateful agent workflows from scratch.\n\n## Defining Your State Schema\n\nThe foundation of any LangGraph workflow is its state. The state represents all the data your workflow needs to operate:\n\n```python\nfrom typing import TypedDict, Annotated\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.message import add_messages\n\nclass WorkflowState(TypedDict):\n    # Messages will be appended, not overwritten\n    messages: Annotated[list, add_messages]\n    # Other state variables will overwrite previous values\n    user_query: str\n    search_results: list[str]\n```\n\n## Creating Nodes\n\nNodes are the building blocks of your workflow - they contain the actual logic:\n\n```python\ndef retrieve_information(state: WorkflowState):\n    # Your retrieval logic here\n    results = search_web(state[\"user_query\"])\n    return {\"search_results\": results}\n\ndef generate_response(state: WorkflowState):\n    # Your generation logic here\n    response = llm.invoke({\n        \"messages\": state[\"messages\"],\n        \"context\": state[\"search_results\"]\n    })\n    return {\"messages\": [response]}\n```\n\n## Building the Graph\n\nCombine your nodes into a functional workflow:\n\n```python\nfrom langgraph.graph import StateGraph, START, END\n\n# Initialize the graph builder\nbuilder = StateGraph(WorkflowState)\n\n# Add nodes\nbuilder.add_node(\"retriever\", retrieve_information)\nbuilder.add_node(\"generator\", generate_response)\n\n# Define the workflow path\nbuilder.add_edge(START, \"retriever\")\nbuilder.add_edge(\"retriever\", \"generator\")\nbuilder.add_edge(\"generator\", END)\n\n# Compile the graph\nworkflow = builder.compile()\n```\n\n## Using Reducers for State Management\n\nReducers control how state updates are applied:\n\n```python\nfrom operator import add\n\nclass StateWithCounter(TypedDict):\n    messages: Annotated[list, add_messages]\n    call_count: Annotated[int, add]  # Will sum values\n\ndef counting_node(state: StateWithCounter):\n    return {\"call_count\": 1}  # Each call adds 1 to the total\n```\n\n## Conditional Workflows\n\nAdd branching logic to your workflows:\n\n```python\ndef should_retry(state: WorkflowState):\n    if needs_more_info(state[\"messages\"][-1]):\n        return \"retriever\"\n    return END\n\nbuilder.add_conditional_edges(\n    \"generator\",\n    should_retry,\n    {\"retriever\": \"retriever\", END: END}\n)\n```\n\n## Compiling and Running\n\nFinalize and test your workflow:\n\n```python\n# Invoke with initial state\nresult = workflow.invoke({\n    \"user_query\": \"What's the weather in SF?\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in SF?\"}]\n})\n\n# Stream intermediate steps\nfor step in workflow.stream(initial_state, stream_mode=\"updates\"):\n    print(step)\n```\n\nThis framework gives you complete control to design sophisticated agent workflows while LangGraph handles the state management and execution logic."
    },
    {
      "title": "Advanced Features",
      "brief_description": "Exploring durable execution, human-in-the-loop, and memory capabilities",
      "content": "```markdown\n# Advanced Features\n\nLangGraph provides several powerful features that enable building sophisticated, production-ready agents. In this section, we'll explore three key capabilities: durable execution, human-in-the-loop workflows, and memory management.\n\n## Durable Execution\n\nDurable execution allows your agents to persist through failures and run for extended periods by automatically saving state at key points. This means:\n\n- Agents can resume exactly where they left off after interruptions\n- Long-running workflows can span hours or days\n- State is preserved across system restarts\n\n**Key components:**\n- `Checkpointer`: Saves and restores agent state\n- `StateGraph`: Manages the workflow state schema\n- `Command`: Handles state updates and control flow\n\nExample of enabling durable execution:\n```python\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# Create a checkpointer\ncheckpointer = MemorySaver()\n\n# Compile graph with checkpointer\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Run with thread_id for state tracking\nconfig = {\"configurable\": {\"thread_id\": \"123\"}}\ngraph.invoke(inputs, config=config)\n```\n\n## Human-in-the-Loop\n\nLangGraph makes it easy to incorporate human oversight into your agent workflows:\n\n- Pause execution at any point for human review\n- Modify agent state before continuing\n- Approve or reject actions like API calls\n\n**Implementation patterns:**\n1. **Approval Gates**: Interrupt before critical actions\n2. **State Inspection**: Allow humans to view and modify state\n3. **Tool Validation**: Review tool calls before execution\n\nExample of adding an approval checkpoint:\n```python\ndef should_approve(state):\n    return \"approve\" if human_review_passed() else \"reject\"\n\nbuilder.add_conditional_edges(\n    \"critical_node\",\n    should_approve,\n    {\"approve\": \"next_node\", \"reject\": \"correction_node\"}\n)\n```\n\n## Memory Management\n\nLangGraph provides comprehensive memory capabilities:\n\n**Short-term Memory**:\n- Maintains conversation history within a session\n- Uses message reducers to manage context window size\n- Persisted via checkpointer\n\n**Long-term Memory**:\n- Stores information across sessions\n- Supports semantic search over memories\n- Organized by custom namespaces\n\nExample memory configuration:\n```python\nfrom typing import Annotated\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    # Short-term memory\n    messages: Annotated[list, add_messages]\n    \n    # Long-term memory key\n    user_profile: dict\n\n# Access memories in nodes\ndef recall_memory(state: State):\n    relevant_memories = memory_store.search(\n        namespace=(\"user123\", \"preferences\"),\n        query=state[\"messages\"][-1].content\n    )\n    return {\"context\": relevant_memories}\n```\n\nThese advanced features enable you to build robust, adaptable agents that can handle complex real-world scenarios while maintaining control and observability.\n```"
    },
    {
      "title": "Deployment and Monitoring",
      "brief_description": "Guidance on deploying agents and using LangSmith for observability",
      "content": "```markdown\n# Deployment and Monitoring\n\n## Deploying Your LangGraph Agents\n\nOnce you've built your agent, you'll want to deploy it for production use. LangGraph provides several deployment options:\n\n### Local Deployment\nFor testing and development, you can run your agent locally:\n```python\ngraph = builder.compile()\nresult = graph.invoke({\"input\": \"your_input\"})\n```\n\n### LangGraph Platform\nFor production deployments, consider using the LangGraph Platform which provides:\n- Scalable infrastructure for long-running agents\n- Built-in persistence and checkpointing\n- Easy integration with LangSmith\n\n```python\n# Deploy using the LangGraph CLI\nlanggraph deploy my_agent.py\n```\n\n### Container Deployment\nPackage your agent as a Docker container for flexible deployment:\n```dockerfile\nFROM python:3.11\nRUN pip install langgraph\nCOPY your_agent.py .\nCMD [\"python\", \"your_agent.py\"]\n```\n\n## Monitoring with LangSmith\n\nLangSmith provides powerful observability tools for your agents:\n\n### Tracing Agent Execution\n- View complete execution traces of your agent's decisions\n- Inspect state changes at each step\n- Visualize the graph traversal path\n\n### Performance Monitoring\n- Track latency and token usage\n- Identify bottlenecks in your workflow\n- Monitor tool call success rates\n\n### Debugging Tools\n- Replay specific executions\n- Compare different runs\n- Inspect intermediate states\n\n```python\n# Configure LangSmith (add to your agent setup)\nimport os\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"my_agent_project\"\n```\n\n## Best Practices for Production\n\n1. **Set Recursion Limits**: Prevent infinite loops\n   ```python\n   graph.invoke(inputs, config={\"recursion_limit\": 25})\n   ```\n\n2. **Implement Checkpointing**: For long-running agents\n   ```python\n   from langgraph.checkpoint.memory import MemorySaver\n   graph = builder.compile(checkpointer=MemorySaver())\n   ```\n\n3. **Enable Human Review**: For critical decisions\n   ```python\n   graph = builder.compile(interrupt_after=[\"approval_node\"])\n   ```\n\n4. **Monitor Key Metrics**:\n   - Success/failure rates\n   - Average completion time\n   - Tool usage statistics\n\nBy following these deployment and monitoring practices, you can ensure your LangGraph agents run reliably in production while maintaining visibility into their performance and behavior.\n```"
    }
  ],
  "full_markdown": "# Getting Started with LangGraph: Building Stateful Agents\n\n*Generated on: 2025-07-03 15:41:30*\n\n*Source: Create a tutorial from the documentation at https://langchain-ai.github.io/langgraph/*\n\n---\n\n## 1. Introduction to LangGraph\n\n```markdown\n# Introduction to LangGraph\n\n## What is LangGraph?\n\nLangGraph is a powerful orchestration framework designed for building stateful, long-running agents and workflows. Trusted by companies like Klarna, Replit, and Elastic, it provides the foundation for creating sophisticated AI systems that can maintain context, handle complex tasks, and interact with users over extended periods.\n\n## Core Benefits\n\nLangGraph offers several key advantages for agent development:\n\n1. **Durable Execution**: Build agents that persist through failures and can run for extended periods, automatically resuming from where they left off.\n\n2. **Human-in-the-Loop**: Seamlessly incorporate human oversight by inspecting and modifying agent state at any point during execution.\n\n3. **Comprehensive Memory**: Create truly stateful agents with both short-term working memory and long-term persistent memory across sessions.\n\n4. **Production-Ready**: Deploy sophisticated agent systems with scalable infrastructure designed for stateful workflows.\n\n## Ecosystem Positioning\n\nWhile LangGraph can be used standalone, it integrates seamlessly with the broader LangChain ecosystem:\n\n- **LangChain**: Provides integrations and composable components for LLM application development\n- **LangSmith**: Offers debugging and observability tools for complex agent behavior\n- **LangGraph Platform**: A deployment platform for managing and scaling agents in production\n\nLangGraph is particularly well-suited for applications requiring:\n- Multi-step reasoning\n- Dynamic workflow adaptation\n- Persistent conversation history\n- Human approval workflows\n- Integration with external tools and APIs\n\n## Key Concepts\n\nAt its core, LangGraph models agent workflows as graphs composed of:\n- **State**: A shared data structure representing the current system snapshot\n- **Nodes**: Functions containing your agent's logic\n- **Edges**: Connections that determine the flow between nodes\n\nThis architecture enables you to build complex, looping workflows that evolve state over time while maintaining full control over your agent's behavior.\n```\n\n## 2. Installation and Setup\n\n```markdown\n# Installation and Setup\n\n## Prerequisites\n\nBefore installing LangGraph, ensure you have:\n- Python 3.11 or later\n- pip package manager\n- An API key for your preferred LLM provider (Anthropic, OpenAI, etc.)\n\n## Installing LangGraph\n\nInstall the core LangGraph package using pip:\n\n```bash\npip install -U langgraph\n```\n\nFor full functionality including LLM integrations, install with optional dependencies:\n\n```bash\npip install -U \"langgraph[anthropic]\"  # For Anthropic models\n# or\npip install -U \"langgraph[openai]\"     # For OpenAI models\n```\n\n## Verifying Your Installation\n\nAfter installation, verify everything works by creating a simple agent:\n\n```python\nfrom langgraph.prebuilt import create_react_agent\n\n# Simple tool example\ndef dummy_tool(query: str) -> str:\n    return f\"Processed: {query}\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[dummy_tool],\n    prompt=\"You are a helpful assistant\"\n)\n\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"test message\"}]}\n)\nprint(response)\n```\n\n## Configuration\n\nSet your API keys as environment variables:\n\n```bash\nexport ANTHROPIC_API_KEY='your-api-key'\n# or\nexport OPENAI_API_KEY='your-api-key'\n```\n\n## Troubleshooting\n\nIf you encounter issues:\n1. Check Python version (`python --version`)\n2. Verify pip is up to date (`pip install --upgrade pip`)\n3. Ensure your API keys are properly set\n4. Consult the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) for version-specific notes\n\nFor production environments, consider using a dependency manager like `uv` for faster, more reliable installations:\n\n```bash\nuv pip install --python=3.11 \"langgraph[anthropic]\"\n```\n\nNow you're ready to start building with LangGraph!\n```\n\n## 3. Creating Your First Agent\n\n# Creating Your First Agent\n\nIn this section, we'll walk through building your first agent using LangGraph's prebuilt components. This will give you a hands-on introduction to creating stateful agents without needing to build everything from scratch.\n\n## Prerequisites\n\nBefore we begin, ensure you have:\n1. Python 3.11+ installed\n2. LangGraph installed (`pip install -U langgraph`)\n3. An Anthropic API key (or other supported LLM provider)\n\n## Step 1: Import Required Components\n\n```python\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain.chat_models import init_chat_model\n```\n\n## Step 2: Define Your Tools\n\nTools are functions your agent can call to perform actions. Let's create a simple weather lookup tool:\n\n```python\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    if city.lower() == \"san francisco\":\n        return \"Sunny with a high of 65Â°F\"\n    return f\"Weather information unavailable for {city}\"\n```\n\n## Step 3: Initialize Your LLM\n\n```python\n# For Anthropic (requires langchain[anthropic])\nllm = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\", temperature=0)\n```\n\n## Step 4: Create the Agent\n\nCombine your LLM and tools into an agent:\n\n```python\nagent = create_react_agent(\n    model=llm,\n    tools=[get_weather],\n    prompt=\"You are a helpful weather assistant\"\n)\n```\n\n## Step 5: Run Your Agent\n\nInvoke the agent with a user message:\n\n```python\nresponse = agent.invoke({\n    \"messages\": [{\n        \"role\": \"user\", \n        \"content\": \"What's the weather in San Francisco?\"\n    }]\n})\n```\n\n## Understanding the Response\n\nThe agent will return a dictionary containing:\n- `messages`: The conversation history\n- Potentially a `structured_response` if configured\n- Any custom state fields you've defined\n\n## Adding Memory (Optional)\n\nTo enable multi-turn conversations, add a checkpointer:\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\nagent = create_react_agent(\n    model=llm,\n    tools=[get_weather],\n    checkpointer=checkpointer\n)\n\n# Use thread_id to maintain conversation state\nconfig = {\"configurable\": {\"thread_id\": \"123\"}}\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi!\"}]}, config)\n```\n\n## Next Steps\n\nYou've now created a basic agent! In the next sections, we'll explore:\n- Customizing the agent's behavior\n- Building more complex workflows\n- Adding advanced features like human review\n\nTry experimenting with different tools and prompts to see how they affect your agent's behavior.\n\n## 4. Understanding Graph Concepts\n\n# Understanding Graph Concepts\n\nLangGraph models agent workflows as graphs composed of three fundamental components that work together to create stateful, controllable agents. Let's explore each concept in detail:\n\n## State: The Shared Memory\n\nThe `State` is a shared data structure representing your application's current snapshot:\n- Typically defined as a `TypedDict` or Pydantic `BaseModel`\n- Contains all variables your nodes need to access and modify\n- Persists throughout the execution of your graph\n\n```python\nfrom typing import Annotated, TypedDict\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.message import add_messages\n\nclass AgentState(TypedDict):\n    messages: Annotated[list, add_messages]  # Message history\n    user_preferences: dict  # User-specific settings\n    session_data: dict  # Temporary session information\n```\n\n## Nodes: The Workers\n\nNodes are Python functions that contain your agent's logic:\n- Receive the current `State` as input\n- Perform computations or side effects\n- Return updates to the `State`\n\n```python\ndef process_user_input(state: AgentState):\n    \"\"\"Node to handle user input\"\"\"\n    last_message = state[\"messages\"][-1]\n    processed = analyze_message(last_message)\n    return {\"messages\": [processed], \"session_data\": {\"last_processed\": now()}}\n```\n\n## Edges: The Decision Makers\n\nEdges determine the flow between nodes:\n- **Normal edges**: Fixed transitions from one node to another\n- **Conditional edges**: Dynamic routing based on state\n\n```python\nfrom langgraph.graph import StateGraph\n\nbuilder = StateGraph(AgentState)\n\n# Add nodes\nbuilder.add_node(\"process_input\", process_user_input)\nbuilder.add_node(\"generate_response\", generate_response)\n\n# Fixed edge\nbuilder.add_edge(\"process_input\", \"generate_response\")\n\n# Conditional edge\ndef should_escalate(state):\n    if state[\"session_data\"].get(\"needs_human\"):\n        return \"human_review\"\n    return \"generate_response\"\n\nbuilder.add_conditional_edges(\"process_input\", should_escalate)\n```\n\n## Reducers: State Update Handlers\n\nReducers control how state updates are applied:\n- Each state key can have its own reducer\n- Default is overwrite behavior\n- Custom reducers enable merging strategies\n\n```python\nfrom operator import add\n\nclass AgentState(TypedDict):\n    message_count: Annotated[int, add]  # Will sum values\n    message_history: Annotated[list, lambda x,y: x + y]  # Will concatenate\n    current_status: str  # Will overwrite\n```\n\n### Key Takeaways:\n1. **State** is your application's shared memory\n2. **Nodes** contain your business logic\n3. **Edges** control workflow routing\n4. **Reducers** manage state updates\n\nThese components work together to create flexible, stateful agent workflows that can handle complex, multi-step processes while maintaining full control over execution flow.\n\n## 5. Building Custom Workflows\n\n# Building Custom Workflows\n\nLangGraph's true power emerges when you move beyond prebuilt agents and start creating custom workflows tailored to your specific needs. This section will guide you through building stateful agent workflows from scratch.\n\n## Defining Your State Schema\n\nThe foundation of any LangGraph workflow is its state. The state represents all the data your workflow needs to operate:\n\n```python\nfrom typing import TypedDict, Annotated\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.message import add_messages\n\nclass WorkflowState(TypedDict):\n    # Messages will be appended, not overwritten\n    messages: Annotated[list, add_messages]\n    # Other state variables will overwrite previous values\n    user_query: str\n    search_results: list[str]\n```\n\n## Creating Nodes\n\nNodes are the building blocks of your workflow - they contain the actual logic:\n\n```python\ndef retrieve_information(state: WorkflowState):\n    # Your retrieval logic here\n    results = search_web(state[\"user_query\"])\n    return {\"search_results\": results}\n\ndef generate_response(state: WorkflowState):\n    # Your generation logic here\n    response = llm.invoke({\n        \"messages\": state[\"messages\"],\n        \"context\": state[\"search_results\"]\n    })\n    return {\"messages\": [response]}\n```\n\n## Building the Graph\n\nCombine your nodes into a functional workflow:\n\n```python\nfrom langgraph.graph import StateGraph, START, END\n\n# Initialize the graph builder\nbuilder = StateGraph(WorkflowState)\n\n# Add nodes\nbuilder.add_node(\"retriever\", retrieve_information)\nbuilder.add_node(\"generator\", generate_response)\n\n# Define the workflow path\nbuilder.add_edge(START, \"retriever\")\nbuilder.add_edge(\"retriever\", \"generator\")\nbuilder.add_edge(\"generator\", END)\n\n# Compile the graph\nworkflow = builder.compile()\n```\n\n## Using Reducers for State Management\n\nReducers control how state updates are applied:\n\n```python\nfrom operator import add\n\nclass StateWithCounter(TypedDict):\n    messages: Annotated[list, add_messages]\n    call_count: Annotated[int, add]  # Will sum values\n\ndef counting_node(state: StateWithCounter):\n    return {\"call_count\": 1}  # Each call adds 1 to the total\n```\n\n## Conditional Workflows\n\nAdd branching logic to your workflows:\n\n```python\ndef should_retry(state: WorkflowState):\n    if needs_more_info(state[\"messages\"][-1]):\n        return \"retriever\"\n    return END\n\nbuilder.add_conditional_edges(\n    \"generator\",\n    should_retry,\n    {\"retriever\": \"retriever\", END: END}\n)\n```\n\n## Compiling and Running\n\nFinalize and test your workflow:\n\n```python\n# Invoke with initial state\nresult = workflow.invoke({\n    \"user_query\": \"What's the weather in SF?\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in SF?\"}]\n})\n\n# Stream intermediate steps\nfor step in workflow.stream(initial_state, stream_mode=\"updates\"):\n    print(step)\n```\n\nThis framework gives you complete control to design sophisticated agent workflows while LangGraph handles the state management and execution logic.\n\n## 6. Advanced Features\n\n```markdown\n# Advanced Features\n\nLangGraph provides several powerful features that enable building sophisticated, production-ready agents. In this section, we'll explore three key capabilities: durable execution, human-in-the-loop workflows, and memory management.\n\n## Durable Execution\n\nDurable execution allows your agents to persist through failures and run for extended periods by automatically saving state at key points. This means:\n\n- Agents can resume exactly where they left off after interruptions\n- Long-running workflows can span hours or days\n- State is preserved across system restarts\n\n**Key components:**\n- `Checkpointer`: Saves and restores agent state\n- `StateGraph`: Manages the workflow state schema\n- `Command`: Handles state updates and control flow\n\nExample of enabling durable execution:\n```python\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# Create a checkpointer\ncheckpointer = MemorySaver()\n\n# Compile graph with checkpointer\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Run with thread_id for state tracking\nconfig = {\"configurable\": {\"thread_id\": \"123\"}}\ngraph.invoke(inputs, config=config)\n```\n\n## Human-in-the-Loop\n\nLangGraph makes it easy to incorporate human oversight into your agent workflows:\n\n- Pause execution at any point for human review\n- Modify agent state before continuing\n- Approve or reject actions like API calls\n\n**Implementation patterns:**\n1. **Approval Gates**: Interrupt before critical actions\n2. **State Inspection**: Allow humans to view and modify state\n3. **Tool Validation**: Review tool calls before execution\n\nExample of adding an approval checkpoint:\n```python\ndef should_approve(state):\n    return \"approve\" if human_review_passed() else \"reject\"\n\nbuilder.add_conditional_edges(\n    \"critical_node\",\n    should_approve,\n    {\"approve\": \"next_node\", \"reject\": \"correction_node\"}\n)\n```\n\n## Memory Management\n\nLangGraph provides comprehensive memory capabilities:\n\n**Short-term Memory**:\n- Maintains conversation history within a session\n- Uses message reducers to manage context window size\n- Persisted via checkpointer\n\n**Long-term Memory**:\n- Stores information across sessions\n- Supports semantic search over memories\n- Organized by custom namespaces\n\nExample memory configuration:\n```python\nfrom typing import Annotated\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    # Short-term memory\n    messages: Annotated[list, add_messages]\n    \n    # Long-term memory key\n    user_profile: dict\n\n# Access memories in nodes\ndef recall_memory(state: State):\n    relevant_memories = memory_store.search(\n        namespace=(\"user123\", \"preferences\"),\n        query=state[\"messages\"][-1].content\n    )\n    return {\"context\": relevant_memories}\n```\n\nThese advanced features enable you to build robust, adaptable agents that can handle complex real-world scenarios while maintaining control and observability.\n```\n\n## 7. Deployment and Monitoring\n\n```markdown\n# Deployment and Monitoring\n\n## Deploying Your LangGraph Agents\n\nOnce you've built your agent, you'll want to deploy it for production use. LangGraph provides several deployment options:\n\n### Local Deployment\nFor testing and development, you can run your agent locally:\n```python\ngraph = builder.compile()\nresult = graph.invoke({\"input\": \"your_input\"})\n```\n\n### LangGraph Platform\nFor production deployments, consider using the LangGraph Platform which provides:\n- Scalable infrastructure for long-running agents\n- Built-in persistence and checkpointing\n- Easy integration with LangSmith\n\n```python\n# Deploy using the LangGraph CLI\nlanggraph deploy my_agent.py\n```\n\n### Container Deployment\nPackage your agent as a Docker container for flexible deployment:\n```dockerfile\nFROM python:3.11\nRUN pip install langgraph\nCOPY your_agent.py .\nCMD [\"python\", \"your_agent.py\"]\n```\n\n## Monitoring with LangSmith\n\nLangSmith provides powerful observability tools for your agents:\n\n### Tracing Agent Execution\n- View complete execution traces of your agent's decisions\n- Inspect state changes at each step\n- Visualize the graph traversal path\n\n### Performance Monitoring\n- Track latency and token usage\n- Identify bottlenecks in your workflow\n- Monitor tool call success rates\n\n### Debugging Tools\n- Replay specific executions\n- Compare different runs\n- Inspect intermediate states\n\n```python\n# Configure LangSmith (add to your agent setup)\nimport os\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"my_agent_project\"\n```\n\n## Best Practices for Production\n\n1. **Set Recursion Limits**: Prevent infinite loops\n   ```python\n   graph.invoke(inputs, config={\"recursion_limit\": 25})\n   ```\n\n2. **Implement Checkpointing**: For long-running agents\n   ```python\n   from langgraph.checkpoint.memory import MemorySaver\n   graph = builder.compile(checkpointer=MemorySaver())\n   ```\n\n3. **Enable Human Review**: For critical decisions\n   ```python\n   graph = builder.compile(interrupt_after=[\"approval_node\"])\n   ```\n\n4. **Monitor Key Metrics**:\n   - Success/failure rates\n   - Average completion time\n   - Tool usage statistics\n\nBy following these deployment and monitoring practices, you can ensure your LangGraph agents run reliably in production while maintaining visibility into their performance and behavior.\n```\n\n"
}